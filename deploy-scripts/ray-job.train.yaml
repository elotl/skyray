apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: rayjob-train
  labels:
    app.kubernetes.io/instance: rayjob
spec:
  # submissionMode specifies how RayJob submits the Ray job to the RayCluster.
  # The default value is "K8sJobMode", meaning RayJob will submit the Ray job via a submitter Kubernetes Job.
  # The alternative value is "HTTPMode", indicating that KubeRay will submit the Ray job by sending an HTTP request to the RayCluster.
  # submissionMode: "K8sJobMode"
  entrypoint: python /home/ray/samples/train_code.py
  # shutdownAfterJobFinishes specifies whether the RayCluster should be deleted after the RayJob finishes. Default is false.
  # shutdownAfterJobFinishes: false

  # ttlSecondsAfterFinished specifies the number of seconds after which the RayCluster will be deleted after the RayJob finishes.
  # ttlSecondsAfterFinished: 10

  # activeDeadlineSeconds is the duration in seconds that the RayJob may be active before
  # KubeRay actively tries to terminate the RayJob; value must be positive integer.
  # activeDeadlineSeconds: 120

  # RuntimeEnvYAML represents the runtime environment configuration provided as a multi-line YAML string.
  # See https://docs.ray.io/en/latest/ray-core/handling-dependencies.html for details.
  # (New in KubeRay version 1.0.)
  runtimeEnvYAML: |
    pip:
      - requests==2.26.0
      - pendulum==2.1.2

  # Suspend specifies whether the RayJob controller should create a RayCluster instance.
  # If a job is applied with the suspend field set to true, the RayCluster will not be created and we will wait for the transition to false.
  # If the RayCluster is already created, it will be deleted. In the case of transition to false, a new RayCluste rwill be created.
  # suspend: false

  # rayClusterSpec specifies the RayCluster instance to be created by the RayJob controller.
  rayClusterSpec:
    rayVersion: '2.2.0' # should match the Ray version in the image of the containers
    # Ray head pod template
    headGroupSpec:
      # The `rayStartParams` are used to configure the `ray start` command.
      # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.
      # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.
      rayStartParams:
        dashboard-host: '0.0.0.0'
      #pod template
      template:
        spec:
          containers:
            - name: ray-head
              image: rayproject/ray-ml:2.2.0-gpu
              env:
                - name: AWS_ACCESS_KEY_ID
                  value: ${AWS_ACCESS_KEY_ID}
                - name: AWS_SECRET_ACCESS_KEY
                  value: ${AWS_SECRET_ACCESS_KEY}
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265 # Ray dashboard
                  name: dashboard
                - containerPort: 10001
                  name: client
              resources:
                limits:
                  cpu: "4"
                  memory: "24G"
                requests:
                  cpu: "4"
                  memory: "12G"
              volumeMounts:
                - mountPath: /home/ray/samples
                  name: code-train
          volumes:
            # You set volumes at the Pod level, then mount them into containers inside that Pod
            - name: code-train
              configMap:
                # Provide the name of the ConfigMap you want to mount.
                name: ray-job-code-train
                # An array of keys from the ConfigMap to create as files
                items:
                  - key: train_code.py
                    path: train_code.py
    workerGroupSpecs:
      # the pod replicas in this group typed worker
      - replicas: 2
        minReplicas: 1
        maxReplicas: 300
        # logical group name, for this called small-group, also can be functional
        groupName: small-group
        rayStartParams:
          num-gpus: "1"
        rayStartParams: {}
        #pod template
        template:
          metadata:
            labels:
              key: value
            # annotations for pod
            annotations:
              key: value
          spec:
            tolerations:
              - key: nvidia.com/gpu
                effect: NoSchedule
                operator: Exists
            containers:
              - name: machine-learning # must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc'
                image: rayproject/ray-ml:2.2.0-gpu
                env:
                  - name: AWS_ACCESS_KEY_ID
                    value: ${AWS_ACCESS_KEY_ID}
                  - name: AWS_SECRET_ACCESS_KEY
                    value: ${AWS_SECRET_ACCESS_KEY}
                lifecycle:
                  preStop:
                    exec:
                      command: [ "/bin/sh","-c","ray stop" ]
                resources:
                  limits:
                    cpu: "8"
                    memory: "24G"
                    nvidia.com/gpu: 1
                  requests:
                    cpu: "4"
                    memory: "12G"
                    nvidia.com/gpu: 1
  # SubmitterPodTemplate is the template for the pod that will run the `ray job submit` command against the RayCluster.
  # If SubmitterPodTemplate is specified, the first container is assumed to be the submitter container.
  # submitterPodTemplate:
  #   spec:
  #     restartPolicy: Never
  #     containers:
  #       - name: my-custom-rayjob-submitter-pod
  #         image: rayproject/ray:2.9.0
  #         # If Command is not specified, the correct command will be supplied at runtime using the RayJob spec `entrypoint` field.
  #         # Specifying Command is not recommended.
  #         # command: ["sh", "-c", "ray job submit --address=http://$RAY_DASHBOARD_ADDRESS --submission-id=$RAY_JOB_SUBMISSION_ID -- echo hello world"]


######################Ray codee#################################
# this code is mounted into the container and executed to run example training job
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-job-code-train
  labels:
    app.kubernetes.io/instance: rayjob
data:
  train_code.py: |
    from ray.job_submission import JobSubmissionClient, JobStatus
    import time

    client = JobSubmissionClient("http://127.0.0.1:8265")

    kick_off_pytorch_benchmark = (
        "git clone -b ray-2.2.0 https://github.com/ray-project/ray || true;"
        "python ray/release/air_tests/air_benchmarks/workloads/pytorch_training_e2e.py"
        " --data-size-gb=10 --num-epochs=6 --num-workers=2"
    )

    submission_id = client.submit_job(entrypoint=kick_off_pytorch_benchmark)
    print(submission_id)

    def wait_until_status(job_id, status_to_wait_for, timeout_seconds=600, sleep_seconds=10):
        start = time.time()
        while time.time() - start <= timeout_seconds:
            status = client.get_job_status(job_id)
            print(f"status: {status}")
            if status in status_to_wait_for:
                break
            time.sleep(sleep_seconds)

    wait_until_status(submission_id, {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED})
    logs = client.get_job_logs(submission_id)
    print(logs)
