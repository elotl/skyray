apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: rayjob-sample
spec:
  entrypoint: python /home/ray/samples/sample_code.py
  shutdownAfterJobFinishes: true
  rayClusterSpec:
    rayVersion: '2.41.0'
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        metadata:
          labels:
            elotl-luna: "true"
          annotations:
            "node.elotl.co/instance-gpu-skus": "${RAY_CLUSTER_GPU_SKUS}"
            "node.elotl.co/instance-offerings": "${BATCH_JOB_PRICE_CATEGORIES}"
            "node.elotl.co/instance-max-gpus": "${RAY_CLUSTER_GPU_COUNT}"
            "node.elotl.co/instance-max-cost": "${RAY_CLUSTER_MAX_NODE_COST}"
        spec:
          tolerations:
            - key: "kubernetes.azure.com/scalesetpriority"
              operator: "Equal"
              value: "spot"
              effect: "NoSchedule"
          containers:
            - name: ray-head
              image: rayproject/ray-ml:2.41.0.deprecated-py39-gpu
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265
                  name: dashboard
                - containerPort: 10001
                  name: client
              resources:
                limits:
                  nvidia.com/gpu: "${RAY_CLUSTER_GPU_COUNT}"
                  cpu: "${RAY_CLUSTER_CPU_COUNT}"
                  memory: "${RAY_CLUSTER_MEMORY_SIZE}"
                requests:
                  nvidia.com/gpu: "${RAY_CLUSTER_GPU_COUNT}"
                  cpu: "${RAY_CLUSTER_CPU_COUNT}"
                  memory: "${RAY_CLUSTER_MEMORY_SIZE}"
              volumeMounts:
                - mountPath: /home/ray/samples
                  name: code-sample
          volumes:
            - name: code-sample
              configMap:
                name: ray-job-code-sample
                items:
                  - key: sample_code.py
                    path: sample_code.py
  # SubmitterPodTemplate is the template for the pod that will run the `ray job submit` command against the RayCluster.
  # If SubmitterPodTemplate is specified, the first container is assumed to be the submitter container.
  submitterPodTemplate:
    metadata:
      labels:
        elotl-luna: "true"
      annotations:
        "node.elotl.co/instance-offerings": "${BATCH_JOB_PRICE_CATEGORIES}"
    spec:
      restartPolicy: Never
      tolerations:
        - key: "kubernetes.azure.com/scalesetpriority"
          operator: "Equal"
          value: "spot"
          effect: "NoSchedule"
      containers:
        - name: my-custom-rayjob-submitter-pod
          image: rayproject/ray-ml:2.41.0.deprecated-py39-gpu

######################Ray code #################################
# This sample is from https://docs.ray.io/en/latest/data/examples/huggingface_vit_batch_prediction.html
# It is mounted into the container.
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-job-code-sample
data:
  sample_code.py: |
    import ray

    s3_uri = "s3://anonymous@air-example-data-2/imagenette2/val/"

    ds = ray.data.read_images(
        s3_uri, mode="RGB"
    )
    ds
    from typing import Dict
    import numpy as np

    from transformers import pipeline
    from PIL import Image

    BATCH_SIZE = 16

    class ImageClassifier:
        def __init__(self):
            # If doing CPU inference, set `device="cpu"` instead.
            self.classifier = pipeline("image-classification", model="google/vit-base-patch16-224", device=0)

        def __call__(self, batch: Dict[str, np.ndarray]):
            # Convert the numpy array of images into a list of PIL images which is the format the HF pipeline expects.
            outputs = self.classifier(
                [Image.fromarray(image_array) for image_array in batch["image"]],
                top_k=1,
                batch_size=BATCH_SIZE)

            # `outputs` is a list of length-one lists. For example:
            # [[{'score': '...', 'label': '...'}], ..., [{'score': '...', 'label': '...'}]]
            batch["score"] = [output[0]["score"] for output in outputs]
            batch["label"] = [output[0]["label"] for output in outputs]
            return batch

    predictions = ds.map_batches(
        ImageClassifier,
        compute=ray.data.ActorPoolStrategy(size=${RAY_CLUSTER_GPU_COUNT}), # Change this number based on the number of GPUs in your cluster.
        num_gpus=1, # Specify 1 GPU per model replica.
        batch_size=BATCH_SIZE # Use the largest batch size that can fit on our GPUs
    )

    prediction_batch = predictions.take_batch(5)

    from PIL import Image
    print("A few sample predictions: ")
    for image, prediction in zip(prediction_batch["image"], prediction_batch["label"]):
        img = Image.fromarray(image)
        # Display the image
        img.show()
        print("Label: ", prediction)

    # Write to local disk, or external storage, e.g. S3
    # ds.write_parquet("s3://my_bucket/my_folder")
